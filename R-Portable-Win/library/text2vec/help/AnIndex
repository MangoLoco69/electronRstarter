as.lda_c	as.lda_c
BNS	BNS
char_tokenizer	tokenizers
check_analogy_accuracy	check_analogy_accuracy
coherence	coherence
Collocations	Collocations
combine_vocabularies	combine_vocabularies
create_dtm	create_dtm
create_dtm.itoken	create_dtm
create_dtm.itoken_parallel	create_dtm
create_tcm	create_tcm
create_tcm.itoken	create_tcm
create_tcm.itoken_parallel	create_tcm
create_vocabulary	create_vocabulary
create_vocabulary.character	create_vocabulary
create_vocabulary.itoken	create_vocabulary
create_vocabulary.itoken_parallel	create_vocabulary
dist2	distances
distances	distances
fit	reexports
fit_transform	reexports
GlobalVectors	GloVe
GloVe	GloVe
hash_vectorizer	vectorizers
idir	ifiles
ifiles	ifiles
ifiles_parallel	ifiles
itoken	itoken
itoken.character	itoken
itoken.iterator	itoken
itoken.list	itoken
itoken_parallel	itoken
itoken_parallel.character	itoken
itoken_parallel.iterator	itoken
itoken_parallel.list	itoken
jsPCA_robust	jsPCA_robust
LatentDirichletAllocation	LatentDirichletAllocation
LatentSemanticAnalysis	LatentSemanticAnalysis
LDA	LatentDirichletAllocation
LSA	LatentSemanticAnalysis
movie_review	movie_review
normalize	normalize
pdist2	distances
perplexity	perplexity
postag_lemma_tokenizer	tokenizers
prepare_analogy_questions	prepare_analogy_questions
prune_vocabulary	prune_vocabulary
psim2	similarities
reexports	reexports
RelaxedWordMoversDistance	RelaxedWordMoversDistance
RWMD	RelaxedWordMoversDistance
sim2	similarities
similarities	similarities
space_tokenizer	tokenizers
split_into	split_into
text2vec	text2vec
TfIdf	TfIdf
tokenizers	tokenizers
vectorizers	vectorizers
vocabulary	create_vocabulary
vocab_vectorizer	vectorizers
word_tokenizer	tokenizers
